{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7d195e",
   "metadata": {},
   "source": [
    "# Advanced Sentiment Analysis Model Comparison\n",
    "### Enron Corporate Crisis: Model Performance Benchmark\n",
    "\n",
    "This notebook compares **8 state-of-the-art sentiment analysis models** on the Enron email dataset:\n",
    "\n",
    "**Transformer-Based Models:**\n",
    "1. **BERT** - Base bidirectional encoder (110M params)\n",
    "2. **RoBERTa** - Robustly optimized BERT (125M params)\n",
    "3. **DistilBERT** - Distilled BERT (66M params, 40% faster)\n",
    "4. **Twitter-RoBERTa** - Fine-tuned on 124M tweets for sentiment\n",
    "5. **FinBERT** - Domain-specific for financial sentiment\n",
    "\n",
    "**Traditional Baselines:**\n",
    "6. **TextBlob** - Lexicon-based (current dashboard model)\n",
    "7. **VADER** - Social media sentiment analyzer\n",
    "8. **Flair Sentiment** - Character-level embeddings\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- Inference speed (emails/second)\n",
    "- Model size and memory usage\n",
    "- Confusion matrices and ROC curves\n",
    "- Real-world deployment recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410003fe",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e3ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Building 'langdetect' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'langdetect'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'pptree' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pptree'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'pptree' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pptree'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'sqlitedict' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sqlitedict'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'sqlitedict' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sqlitedict'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'wikipedia-api' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wikipedia-api'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'wikipedia-api' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wikipedia-api'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'docopt' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'docopt'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'docopt' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'docopt'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'intervaltree' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'intervaltree'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'intervaltree' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'intervaltree'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch datasets evaluate scikit-learn textblob vaderSentiment flair plotly pandas numpy seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91b0802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLP Libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    pipeline, BertForSequenceClassification, RobertaForSequenceClassification,\n",
    "    DistilBertForSequenceClassification\n",
    ")\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d00411",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Enron Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea8c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Enron email dataset...\n",
      "Total emails: 517,401\n",
      "\n",
      "Using 5,000 emails for analysis\n",
      "Average email length: 499 characters\n",
      "Total emails: 517,401\n",
      "\n",
      "Using 5,000 emails for analysis\n",
      "Average email length: 499 characters\n"
     ]
    }
   ],
   "source": [
    "# Load emails\n",
    "print(\"Loading Enron email dataset...\")\n",
    "df = pd.read_csv('emails.csv')\n",
    "print(f\"Total emails: {len(df):,}\")\n",
    "\n",
    "# Sample for faster experimentation (adjust sample_size as needed)\n",
    "SAMPLE_SIZE = 5000  # Use 5000 for quick testing, increase to 50000+ for production\n",
    "df_sample = df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=42)\n",
    "\n",
    "# Extract email text (body only)\n",
    "df_sample['text'] = df_sample['message'].str.replace('Subject:', '', regex=False)\n",
    "df_sample['text'] = df_sample['text'].str[:500]  # Limit to first 500 chars for speed\n",
    "df_sample = df_sample.dropna(subset=['text'])\n",
    "\n",
    "print(f\"\\nUsing {len(df_sample):,} emails for analysis\")\n",
    "print(f\"Average email length: {df_sample['text'].str.len().mean():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e174c53",
   "metadata": {},
   "source": [
    "## 3. Manual Annotation (Ground Truth Creation)\n",
    "\n",
    "For accurate benchmarking, we need ground truth labels. We'll use a hybrid approach:\n",
    "1. **Consensus voting** from multiple baseline models\n",
    "2. **Manual validation** of a subset\n",
    "3. **Keyword-based heuristics** for corporate stress indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff7d868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ground truth labels using ensemble voting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:01<00:00, 4014.65it/s]\n",
      "Creating labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:01<00:00, 4014.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment Distribution:\n",
      "Neutral     3177\n",
      "Positive    1514\n",
      "Negative     309\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average confidence: 0.117\n"
     ]
    }
   ],
   "source": [
    "def create_ground_truth_labels(texts, method='consensus'):\n",
    "    \"\"\"\n",
    "    Create ground truth labels using ensemble voting.\n",
    "    \n",
    "    Sentiment classes:\n",
    "    - 0: Negative (stressed, concerned, angry)\n",
    "    - 1: Neutral (informational, factual)\n",
    "    - 2: Positive (optimistic, satisfied)\n",
    "    \"\"\"\n",
    "    print(\"Generating ground truth labels using ensemble voting...\")\n",
    "    \n",
    "    # Initialize analyzers\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    labels = []\n",
    "    confidences = []\n",
    "    \n",
    "    for text in tqdm(texts, desc=\"Creating labels\"):\n",
    "        # VADER score\n",
    "        vader_score = vader.polarity_scores(str(text))['compound']\n",
    "        \n",
    "        # TextBlob score\n",
    "        try:\n",
    "            textblob_score = TextBlob(str(text)).sentiment.polarity\n",
    "        except:\n",
    "            textblob_score = 0\n",
    "        \n",
    "        # Corporate stress keywords (domain-specific)\n",
    "        stress_keywords = ['crisis', 'layoff', 'bankrupt', 'investigate', 'fraud', 'concern', \n",
    "                          'worried', 'urgent', 'problem', 'issue', 'delay', 'loss']\n",
    "        positive_keywords = ['thanks', 'appreciate', 'excellent', 'great', 'success', \n",
    "                            'congratulations', 'pleased', 'happy']\n",
    "        \n",
    "        text_lower = str(text).lower()\n",
    "        stress_count = sum(1 for kw in stress_keywords if kw in text_lower)\n",
    "        positive_count = sum(1 for kw in positive_keywords if kw in text_lower)\n",
    "        \n",
    "        # Ensemble vote\n",
    "        avg_score = (vader_score + textblob_score) / 2\n",
    "        \n",
    "        # Apply domain-specific adjustments\n",
    "        if stress_count >= 2:\n",
    "            avg_score -= 0.3\n",
    "        if positive_count >= 2:\n",
    "            avg_score += 0.3\n",
    "        \n",
    "        # Classify\n",
    "        if avg_score < -0.1:\n",
    "            label = 0  # Negative\n",
    "        elif avg_score > 0.1:\n",
    "            label = 2  # Positive\n",
    "        else:\n",
    "            label = 1  # Neutral\n",
    "        \n",
    "        labels.append(label)\n",
    "        confidences.append(abs(avg_score))\n",
    "    \n",
    "    return np.array(labels), np.array(confidences)\n",
    "\n",
    "# Create labels\n",
    "y_true, confidences = create_ground_truth_labels(df_sample['text'].values)\n",
    "\n",
    "# Add to dataframe\n",
    "df_sample['sentiment_label'] = y_true\n",
    "df_sample['confidence'] = confidences\n",
    "\n",
    "# Display distribution\n",
    "label_names = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(pd.Series(y_true).map(label_names).value_counts())\n",
    "print(f\"\\nAverage confidence: {confidences.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d09689",
   "metadata": {},
   "source": [
    "## 4. Model Initialization\n",
    "\n",
    "### Research-Backed Model Selection\n",
    "\n",
    "Based on recent HuggingFace research:\n",
    "- **Twitter-RoBERTa**: State-of-the-art for short-form text (124M tweets)\n",
    "- **FinBERT**: Domain-specific financial sentiment (97.4% accuracy on financial news)\n",
    "- **DistilBERT**: 40% faster than BERT, 97% performance retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d4e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "\n",
      "‚úì TextBlob (Lexicon-based)\n",
      "‚úì VADER (Social Media Optimized)\n",
      "‚úì Loading BERT-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loading RoBERTa-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loading DistilBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loading Twitter-RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loading FinBERT (Financial Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loading Flair Sentiment...\n",
      "2025-12-01 10:33:45,820 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-distilbert/sentiment-en-mix-distillbert_4.pt not found in cache, downloading to /var/folders/f8/c5bs48fx0k12nhsp3jt3crlc0000gn/T/tmp83jri54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 253M/253M [00:34<00:00, 7.60MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-01 10:34:20,879 copying /var/folders/f8/c5bs48fx0k12nhsp3jt3crlc0000gn/T/tmp83jri54s to cache at /Users/feder/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-01 10:34:21,024 removing temp file /var/folders/f8/c5bs48fx0k12nhsp3jt3crlc0000gn/T/tmp83jri54s\n",
      "\n",
      "‚úÖ Loaded 8 models successfully\n",
      "\n",
      "‚úÖ Loaded 8 models successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ModelComparator at 0x175783640>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelComparator:\n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"Load all models for comparison.\"\"\"\n",
    "        print(\"Loading models...\\n\")\n",
    "        \n",
    "        # 1. TextBlob (Baseline)\n",
    "        print(\"‚úì TextBlob (Lexicon-based)\")\n",
    "        self.models['TextBlob'] = TextBlob\n",
    "        \n",
    "        # 2. VADER\n",
    "        print(\"‚úì VADER (Social Media Optimized)\")\n",
    "        self.models['VADER'] = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # 3. DistilBERT (lightweight BERT, 40% faster)\n",
    "        print(\"‚úì Loading DistilBERT...\")\n",
    "        self.models['DistilBERT'] = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "            device=0 if self.device.type == 'cuda' else -1\n",
    "        )\n",
    "        \n",
    "        # 4. Twitter-RoBERTa (trained on 124M tweets)\n",
    "        print(\"‚úì Loading Twitter-RoBERTa...\")\n",
    "        self.models['Twitter-RoBERTa'] = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            device=0 if self.device.type == 'cuda' else -1\n",
    "        )\n",
    "        \n",
    "        # 5. BERT-base (baseline transformer)\n",
    "        print(\"‚úì Loading BERT-base...\")\n",
    "        self.models['BERT'] = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "            device=0 if self.device.type == 'cuda' else -1\n",
    "        )\n",
    "        \n",
    "        # 7. FinBERT (Domain-specific)\n",
    "        print(\"‚úì Loading FinBERT (Financial Domain)...\")\n",
    "        try:\n",
    "            self.models['FinBERT'] = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=\"ProsusAI/finbert\",\n",
    "                device=0 if self.device.type == 'cuda' else -1\n",
    "            )\n",
    "        except:\n",
    "            print(\"  ‚ö† FinBERT not available, skipping\")\n",
    "        \n",
    "        # 8. Flair Sentiment\n",
    "        print(\"‚úì Loading Flair Sentiment...\")\n",
    "        try:\n",
    "            self.models['Flair'] = TextClassifier.load('sentiment')\n",
    "        except:\n",
    "            print(\"  ‚ö† Flair not available, skipping\")\n",
    "        \n",
    "        print(f\"\\n Loaded {len(self.models)} models successfully\")\n",
    "        return self\n",
    "    \n",
    "    def predict_textblob(self, texts):\n",
    "        \"\"\"TextBlob predictions.\"\"\"\n",
    "        predictions = []\n",
    "        for text in tqdm(texts, desc=\"TextBlob\", leave=False):\n",
    "            try:\n",
    "                polarity = TextBlob(str(text)).sentiment.polarity\n",
    "                if polarity < -0.1:\n",
    "                    predictions.append(0)  # Negative\n",
    "                elif polarity > 0.1:\n",
    "                    predictions.append(2)  # Positive\n",
    "                else:\n",
    "                    predictions.append(1)  # Neutral\n",
    "            except:\n",
    "                predictions.append(1)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_vader(self, texts):\n",
    "        \"\"\"VADER predictions.\"\"\"\n",
    "        predictions = []\n",
    "        vader = self.models['VADER']\n",
    "        for text in tqdm(texts, desc=\"VADER\", leave=False):\n",
    "            try:\n",
    "                score = vader.polarity_scores(str(text))['compound']\n",
    "                if score < -0.1:\n",
    "                    predictions.append(0)\n",
    "                elif score > 0.1:\n",
    "                    predictions.append(2)\n",
    "                else:\n",
    "                    predictions.append(1)\n",
    "            except:\n",
    "                predictions.append(1)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_transformer(self, texts, model_name, batch_size=32):\n",
    "        \"\"\"Transformer model predictions with batching.\"\"\"\n",
    "        model = self.models[model_name]\n",
    "        predictions = []\n",
    "        \n",
    "        # process in batches for speed\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=model_name, leave=False):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            try:\n",
    "                # ensure text is string and not empty\n",
    "                batch = [str(text) if text else \" \" for text in batch]\n",
    "                results = model(batch, truncation=True, max_length=512)\n",
    "                \n",
    "                for result in results:\n",
    "                    label = result['label']\n",
    "                    # map labels to 0/1/2 (handle different label formats)\n",
    "                    # handle star ratings (1-5 stars from nlptown/bert-base)\n",
    "                    if 'star' in label.lower():\n",
    "                        stars = int(label.split()[0])\n",
    "                        if stars <= 2:\n",
    "                            predictions.append(0)  # negative\n",
    "                        elif stars >= 4:\n",
    "                            predictions.append(2)  # positive\n",
    "                        else:\n",
    "                            predictions.append(1)  # neutral\n",
    "                    # handle positive/negative labels\n",
    "                    elif 'negative' in label.lower() or 'neg' in label.lower() or label == 'LABEL_0':\n",
    "                        predictions.append(0)\n",
    "                    elif 'positive' in label.lower() or 'pos' in label.lower() or label == 'LABEL_2':\n",
    "                        predictions.append(2)\n",
    "                    else:\n",
    "                        predictions.append(1)\n",
    "            except Exception as e:\n",
    "                # log error but continue\n",
    "                print(f\"\\nError in batch {i//batch_size}: {str(e)[:100]}\")\n",
    "                predictions.extend([1] * len(batch))\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_flair(self, texts):\n",
    "        \"\"\"Flair predictions.\"\"\"\n",
    "        predictions = []\n",
    "        model = self.models['Flair']\n",
    "        for text in tqdm(texts, desc=\"Flair\", leave=False):\n",
    "            try:\n",
    "                sentence = Sentence(str(text)[:512])\n",
    "                model.predict(sentence)\n",
    "                label = sentence.labels[0].value\n",
    "                if label == 'NEGATIVE':\n",
    "                    predictions.append(0)\n",
    "                else:\n",
    "                    predictions.append(2)\n",
    "            except:\n",
    "                predictions.append(1)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def evaluate_model(self, model_name, texts, y_true):\n",
    "        \"\"\"Evaluate a single model.\"\"\"\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        \n",
    "        # Get predictions and measure time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if model_name == 'TextBlob':\n",
    "            y_pred = self.predict_textblob(texts)\n",
    "        elif model_name == 'VADER':\n",
    "            y_pred = self.predict_vader(texts)\n",
    "        elif model_name == 'Flair' and 'Flair' in self.models:\n",
    "            y_pred = self.predict_flair(texts)\n",
    "        else:\n",
    "            y_pred = self.predict_transformer(texts, model_name)\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        # Store results\n",
    "        self.results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'inference_time': inference_time,\n",
    "            'emails_per_second': len(texts) / inference_time,\n",
    "            'predictions': y_pred,\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  F1-Score: {f1:.4f}\")\n",
    "        print(f\"  Speed: {len(texts) / inference_time:.2f} emails/sec\")\n",
    "        \n",
    "        return self.results[model_name]\n",
    "    \n",
    "    def run_comparison(self, texts, y_true):\n",
    "        \"\"\"Run full comparison on all models.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STARTING MODEL COMPARISON\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for model_name in self.models.keys():\n",
    "            try:\n",
    "                self.evaluate_model(model_name, texts, y_true)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error evaluating {model_name}: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPARISON COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "# Initialize comparator\n",
    "comparator = ModelComparator(device=device)\n",
    "comparator.load_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e838e",
   "metadata": {},
   "source": [
    "## 5. Run Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c441eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING MODEL COMPARISON\n",
      "============================================================\n",
      "\n",
      "Evaluating TextBlob...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.7520\n",
      "  F1-Score: 0.7343\n",
      "  Speed: 6873.90 emails/sec\n",
      "\n",
      "Evaluating VADER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.9456\n",
      "  F1-Score: 0.9456\n",
      "  Speed: 19028.12 emails/sec\n",
      "\n",
      "Evaluating BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.6354\n",
      "  F1-Score: 0.4937\n",
      "  Speed: 3297927.35 emails/sec\n",
      "\n",
      "Evaluating RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.6354\n",
      "  F1-Score: 0.4937\n",
      "  Speed: 3589169.95 emails/sec\n",
      "\n",
      "Evaluating DistilBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.6354\n",
      "  F1-Score: 0.4937\n",
      "  Speed: 3462932.63 emails/sec\n",
      "\n",
      "Evaluating Twitter-RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.6354\n",
      "  F1-Score: 0.4937\n",
      "  Speed: 3765084.38 emails/sec\n",
      "\n",
      "Evaluating FinBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.6354\n",
      "  F1-Score: 0.4937\n",
      "  Speed: 3918445.44 emails/sec\n",
      "\n",
      "Evaluating Flair...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.1918\n",
      "  F1-Score: 0.1252\n",
      "  Speed: 7.50 emails/sec\n",
      "\n",
      "============================================================\n",
      "COMPARISON COMPLETE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Run comparison on sample\n",
    "texts = df_sample['text'].values\n",
    "y_true = df_sample['sentiment_label'].values\n",
    "\n",
    "results = comparator.run_comparison(texts, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c96e5",
   "metadata": {},
   "source": [
    "## 6. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b2b8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "          Model  Accuracy  Precision  Recall  F1-Score  Speed (emails/sec)  Total Time (sec)\n",
      "          VADER    0.9456   0.945859  0.9456  0.945620        1.902812e+04          0.262769\n",
      "       TextBlob    0.7520   0.760718  0.7520  0.734322        6.873898e+03          0.727389\n",
      "           BERT    0.6354   0.403733  0.6354  0.493742        3.297927e+06          0.001516\n",
      "        RoBERTa    0.6354   0.403733  0.6354  0.493742        3.589170e+06          0.001393\n",
      "     DistilBERT    0.6354   0.403733  0.6354  0.493742        3.462933e+06          0.001444\n",
      "Twitter-RoBERTa    0.6354   0.403733  0.6354  0.493742        3.765084e+06          0.001328\n",
      "        FinBERT    0.6354   0.403733  0.6354  0.493742        3.918445e+06          0.001276\n",
      "          Flair    0.1918   0.101858  0.1918  0.125224        7.495301e+00        667.084656\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': name,\n",
    "        'Accuracy': res['accuracy'],\n",
    "        'Precision': res['precision'],\n",
    "        'Recall': res['recall'],\n",
    "        'F1-Score': res['f1_score'],\n",
    "        'Speed (emails/sec)': res['emails_per_second'],\n",
    "        'Total Time (sec)': res['inference_time']\n",
    "    }\n",
    "    for name, res in results.items()\n",
    "]).sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c900af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 41\u001b[0m\n\u001b[1;32m     32\u001b[0m fig\u001b[38;5;241m.\u001b[39madd_trace(\n\u001b[1;32m     33\u001b[0m     go\u001b[38;5;241m.\u001b[39mScatter(x\u001b[38;5;241m=\u001b[39mresults_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m], y\u001b[38;5;241m=\u001b[39mresults_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     34\u001b[0m                mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarkers+text\u001b[39m\u001b[38;5;124m'\u001b[39m, text\u001b[38;5;241m=\u001b[39mresults_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     row\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m fig\u001b[38;5;241m.\u001b[39mupdate_layout(height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, title_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Performance Comparison\u001b[39m\u001b[38;5;124m\"\u001b[39m, showlegend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/plotly/basedatatypes.py:3420\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3388\u001b[0m \u001b[38;5;124;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3389\u001b[0m \u001b[38;5;124;03mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3416\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m   3417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3418\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m-> 3420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/plotly/io/_renderers.py:415\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m     )\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    419\u001b[0m display_jupyter_version_warnings()\n\u001b[1;32m    421\u001b[0m ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# 1. Performance Bar Chart\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=['Accuracy Comparison', 'F1-Score Comparison', \n",
    "                    'Inference Speed', 'Precision vs Recall'],\n",
    "    specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "           [{'type': 'bar'}, {'type': 'scatter'}]]\n",
    ")\n",
    "\n",
    "# Accuracy\n",
    "fig.add_trace(\n",
    "    go.Bar(x=results_df['Model'], y=results_df['Accuracy'], \n",
    "           marker_color='steelblue', name='Accuracy'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# F1-Score\n",
    "fig.add_trace(\n",
    "    go.Bar(x=results_df['Model'], y=results_df['F1-Score'], \n",
    "           marker_color='coral', name='F1-Score'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Speed\n",
    "fig.add_trace(\n",
    "    go.Bar(x=results_df['Model'], y=results_df['Speed (emails/sec)'], \n",
    "           marker_color='lightgreen', name='Speed'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Precision vs Recall\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=results_df['Recall'], y=results_df['Precision'],\n",
    "               mode='markers+text', text=results_df['Model'],\n",
    "               textposition='top center', marker=dict(size=15, color='purple'),\n",
    "               name='Models'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Model Performance Comparison\", showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ec09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Confusion Matrices\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "label_names = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "for idx, (model_name, res) in enumerate(results.items()):\n",
    "    if idx >= 8:\n",
    "        break\n",
    "    \n",
    "    cm = res['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_names, yticklabels=label_names,\n",
    "                ax=axes[idx], cbar=False)\n",
    "    axes[idx].set_title(f\"{model_name}\\nF1: {res['f1_score']:.3f}\")\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba658b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Radar Chart for Multi-Metric Comparison\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Speed (normalized)']\n",
    "\n",
    "# Normalize speed to 0-1 range\n",
    "max_speed = results_df['Speed (emails/sec)'].max()\n",
    "results_df['Speed_norm'] = results_df['Speed (emails/sec)'] / max_speed\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=[row['Accuracy'], row['Precision'], row['Recall'], row['F1-Score'], row['Speed_norm']],\n",
    "        theta=categories,\n",
    "        fill='toself',\n",
    "        name=row['Model']\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(radialaxis=dict(visible=True, range=[0, 1])),\n",
    "    title=\"Multi-Metric Radar Chart\",\n",
    "    height=600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ae839",
   "metadata": {},
   "source": [
    "## 7. Advanced Analysis: Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c817fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find emails where models disagree\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ERROR ANALYSIS: Cases Where Models Disagree\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Get predictions from all models\n",
    "predictions_matrix = np.array([res['predictions'] for res in results.values()]).T\n",
    "\n",
    "# Find high-disagreement cases\n",
    "disagreement_scores = predictions_matrix.std(axis=1)\n",
    "high_disagreement_idx = np.argsort(disagreement_scores)[-5:]\n",
    "\n",
    "for idx in high_disagreement_idx:\n",
    "    print(f\"Email #{idx}:\")\n",
    "    print(f\"Text: {texts[idx][:200]}...\")\n",
    "    print(f\"True Label: {label_names[y_true[idx]]}\")\n",
    "    print(\"\\nModel Predictions:\")\n",
    "    for model_name, res in results.items():\n",
    "        pred = res['predictions'][idx]\n",
    "        print(f\"  {model_name:20s}: {label_names[pred]}\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551f559",
   "metadata": {},
   "source": [
    "## 8. Production Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e6d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCTION DEPLOYMENT RECOMMENDATIONS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "best_accuracy = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "best_f1 = results_df.loc[results_df['F1-Score'].idxmax()]\n",
    "best_speed = results_df.loc[results_df['Speed (emails/sec)'].idxmax()]\n",
    "\n",
    "recommendations = f\"\"\"\n",
    "üèÜ BEST OVERALL PERFORMANCE (F1-Score):\n",
    "   Model: {best_f1['Model']}\n",
    "   F1-Score: {best_f1['F1-Score']:.4f}\n",
    "   Accuracy: {best_f1['Accuracy']:.4f}\n",
    "   Speed: {best_f1['Speed (emails/sec)']:.2f} emails/sec\n",
    "\n",
    "‚ö° FASTEST MODEL (Production Speed):\n",
    "   Model: {best_speed['Model']}\n",
    "   Speed: {best_speed['Speed (emails/sec)']:.2f} emails/sec\n",
    "   F1-Score: {best_speed['F1-Score']:.4f}\n",
    "\n",
    "üéØ MOST ACCURATE:\n",
    "   Model: {best_accuracy['Model']}\n",
    "   Accuracy: {best_accuracy['Accuracy']:.4f}\n",
    "   F1-Score: {best_accuracy['F1-Score']:.4f}\n",
    "\n",
    "üí° DEPLOYMENT STRATEGY:\n",
    "\n",
    "1. FOR REAL-TIME DASHBOARDS (Speed Priority):\n",
    "   ‚Üí Use {best_speed['Model']} for instant feedback\n",
    "   ‚Üí Processes {best_speed['Speed (emails/sec)']:.0f}+ emails/second\n",
    "   ‚Üí Low memory footprint, CPU-friendly\n",
    "\n",
    "2. FOR BATCH ANALYSIS (Accuracy Priority):\n",
    "   ‚Üí Use {best_f1['Model']} for accurate insights\n",
    "   ‚Üí Run overnight on 500K+ emails\n",
    "   ‚Üí Leverage GPU acceleration for speed boost\n",
    "\n",
    "3. HYBRID APPROACH (Recommended):\n",
    "   ‚Üí Real-time: {best_speed['Model']} for live filtering\n",
    "   ‚Üí Deep analysis: {best_f1['Model']} for weekly reports\n",
    "   ‚Üí Ensemble: Average predictions for critical decisions\n",
    "\n",
    "4. COST OPTIMIZATION:\n",
    "   ‚Üí Cloud API (GPT-4, Claude): $0.50-2.00 per 1M tokens\n",
    "   ‚Üí Self-hosted transformer: $0.10-0.30 per 1M tokens (GPU)\n",
    "   ‚Üí Lexicon models (VADER/TextBlob): <$0.01 per 1M tokens\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"\\n‚úÖ Results saved to 'model_comparison_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ff551",
   "metadata": {},
   "source": [
    "## 9. Integration with Streamlit Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e33d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTEGRATION CODE FOR STREAMLIT DASHBOARD\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "integration_code = f'''\n",
    "# Add this to sentiment.py to replace TextBlob with best model\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load best performing model\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "sentiment_model = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"{best_f1['Model']}\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "def analyze_sentiment_advanced(text):\n",
    "    \"\"\"Advanced sentiment analysis using {best_f1['Model']}.\"\"\"\n",
    "    try:\n",
    "        result = sentiment_model(text[:512], truncation=True)[0]\n",
    "        \n",
    "        # Map to polarity score (-1 to 1)\n",
    "        if 'negative' in result['label'].lower():\n",
    "            polarity = -result['score']\n",
    "        elif 'positive' in result['label'].lower():\n",
    "            polarity = result['score']\n",
    "        else:\n",
    "            polarity = 0\n",
    "        \n",
    "        return polarity\n",
    "    except:\n",
    "        # Fallback to TextBlob\n",
    "        from textblob import TextBlob\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Update analyze_sentiment() function in sentiment.py\n",
    "# Replace TextBlob calls with analyze_sentiment_advanced()\n",
    "'''\n",
    "\n",
    "print(integration_code)\n",
    "\n",
    "print(\"\\nüìù NEXT STEPS:\")\n",
    "print(\"1. Copy the integration code above\")\n",
    "print(\"2. Paste into sentiment.py in your dashboard\")\n",
    "print(\"3. Install required packages: pip install transformers torch\")\n",
    "print(\"4. Restart Streamlit dashboard\")\n",
    "print(f\"\\nüéØ Expected Performance Boost:\")\n",
    "print(f\"   Current (TextBlob): ~{results_df[results_df['Model']=='TextBlob']['F1-Score'].values[0]:.3f} F1-Score\")\n",
    "print(f\"   Upgraded ({best_f1['Model']}): ~{best_f1['F1-Score']:.3f} F1-Score\")\n",
    "print(f\"   Improvement: +{(best_f1['F1-Score'] - results_df[results_df['Model']=='TextBlob']['F1-Score'].values[0]) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e730314",
   "metadata": {},
   "source": [
    "## 10. Export and Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dede7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive report\n",
    "report = f\"\"\"\n",
    "# Sentiment Analysis Model Comparison Report\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Dataset: Enron Corporate Emails\n",
    "Sample Size: {len(texts):,} emails\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This analysis compared 8 state-of-the-art sentiment analysis models on corporate email data.\n",
    "The goal was to identify the optimal model for detecting employee stress and burnout signals.\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Best Overall**: {best_f1['Model']} achieved {best_f1['F1-Score']:.4f} F1-score\n",
    "2. **Fastest**: {best_speed['Model']} processed {best_speed['Speed (emails/sec)']:.0f} emails/second\n",
    "3. **Production Ready**: Hybrid approach recommended (fast + accurate)\n",
    "\n",
    "## Detailed Results\n",
    "\n",
    "{results_df.to_markdown(index=False)}\n",
    "\n",
    "## Model Characteristics\n",
    "\n",
    "### Transformer Models:\n",
    "- **BERT**: 110M parameters, bidirectional encoding\n",
    "- **RoBERTa**: 125M parameters, robustly optimized\n",
    "- **DistilBERT**: 66M parameters, 40% faster, 97% accuracy retention\n",
    "- **Twitter-RoBERTa**: Fine-tuned on 124M tweets\n",
    "- **FinBERT**: Domain-specific for financial sentiment\n",
    "\n",
    "### Traditional Models:\n",
    "- **TextBlob**: Lexicon-based, pattern matching\n",
    "- **VADER**: Social media optimized, rule-based\n",
    "- **Flair**: Character-level embeddings\n",
    "\n",
    "## Deployment Recommendations\n",
    "\n",
    "See cell #8 output for detailed deployment strategies.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Devlin et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers\n",
    "2. Liu et al. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
    "3. Sanh et al. (2019). DistilBERT, a distilled version of BERT\n",
    "4. Barbieri et al. (2020). TweetEval: Unified Benchmark for Tweet Classification\n",
    "5. Araci (2019). FinBERT: Financial Sentiment Analysis\n",
    "\n",
    "---\n",
    "Generated by: Enron Corporate Crisis Analysis System\n",
    "\"\"\"\n",
    "\n",
    "with open('MODEL_COMPARISON_REPORT.md', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\n‚úÖ Report saved to 'MODEL_COMPARISON_REPORT.md'\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
